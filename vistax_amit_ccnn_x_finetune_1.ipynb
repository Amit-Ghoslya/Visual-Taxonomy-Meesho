{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e09c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tqdm import tqdm\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb94855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "category_attributes = pd.read_parquet('category_attributes.parquet')\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "train_df = train_df.drop(columns=['len'])\n",
    "\n",
    "# Preprocess category-to-attributes dictionary\n",
    "category_to_attributes = {\n",
    "    row['Category']: row['Attribute_list']\n",
    "    for _, row in category_attributes.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f847dba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ghosl\\AppData\\Local\\Temp\\ipykernel_15456\\206101962.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[attr_col] = df[attr_col].apply(lambda x: random.choice(non_na_values) if pd.isna(x) else x)\n",
      "C:\\Users\\Ghosl\\AppData\\Local\\Temp\\ipykernel_15456\\206101962.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[attr_col] = df[attr_col].apply(lambda x: random.choice(non_na_values) if pd.isna(x) else x)\n",
      "C:\\Users\\Ghosl\\AppData\\Local\\Temp\\ipykernel_15456\\206101962.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[attr_col] = df[attr_col].apply(lambda x: random.choice(non_na_values) if pd.isna(x) else x)\n",
      "C:\\Users\\Ghosl\\AppData\\Local\\Temp\\ipykernel_15456\\206101962.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[attr_col] = df[attr_col].apply(lambda x: random.choice(non_na_values) if pd.isna(x) else x)\n",
      "C:\\Users\\Ghosl\\AppData\\Local\\Temp\\ipykernel_15456\\206101962.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[attr_col] = df[attr_col].apply(lambda x: random.choice(non_na_values) if pd.isna(x) else x)\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values with random imputation\n",
    "def random_impute(df, attribute_positions):\n",
    "    for attr_col in attribute_positions.keys():\n",
    "        if df[attr_col].isna().sum() > 0:\n",
    "            non_na_values = df[attr_col].dropna().unique()\n",
    "            df[attr_col] = df[attr_col].apply(lambda x: random.choice(non_na_values) if pd.isna(x) else x)\n",
    "    return df\n",
    "\n",
    "# Apply random imputation for each category\n",
    "for _, row in category_attributes.iterrows():\n",
    "    category = row['Category']\n",
    "    attributes = row['Attribute_list']\n",
    "    attribute_positions = {f'attr_{i+1}': attr_name for i, attr_name in enumerate(attributes)}\n",
    "    category_df = train_df[train_df['Category'] == category]\n",
    "    filled_category_df = random_impute(category_df, attribute_positions)\n",
    "    train_df.update(filled_category_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fc25517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace irrelevant attributes with 'DV' and encode attributes\n",
    "for i in range(1, 11):\n",
    "    attr_col = f'attr_{i}'\n",
    "    train_df[attr_col].fillna('DV', inplace=True)\n",
    "train_df['id'] = train_df['id'].astype('int64')\n",
    "\n",
    "encoders = {}\n",
    "for col in [f'attr_{i}' for i in range(1, 11)]:\n",
    "    encoder = LabelEncoder()\n",
    "    train_df[col] = encoder.fit_transform(train_df[col].astype(str))\n",
    "    encoders[col] = encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37c1ed8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ghosl\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "def create_custom_cnn(input_shape=(128, 128, 3), reduced_dim=512):\n",
    "    model = Sequential([\n",
    "        Conv2D(96, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Conv2D(256, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Conv2D(384, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Conv2D(256, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        GlobalAveragePooling2D(),  # Pool across all spatial dimensions to get a feature vector\n",
    "        Dense(reduced_dim, activation='relu'),\n",
    "        Dropout(0.4)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy')\n",
    "    return model\n",
    "\n",
    "# Initialize custom CNN feature extractor\n",
    "feature_extractor = create_custom_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd4cf1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from images\n",
    "def extract_custom_cnn_features(img_path, target_size=(128, 128)):\n",
    "    img = image.load_img(img_path, target_size=target_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
    "    features = feature_extractor.predict(img_array)\n",
    "    return features.flatten()\n",
    "\n",
    "\n",
    "# Batch-wise image feature extraction\n",
    "def extract_image_features_batch(df, img_dir, output_path, feature_extractor):\n",
    "    features = {}\n",
    "    for img_id in df['id']:\n",
    "        img_path = os.path.join(img_dir, f\"{int(img_id):06}.jpg\")\n",
    "        if os.path.exists(img_path):\n",
    "            features[img_id] = extract_custom_cnn_features(img_path)\n",
    "    features_df = pd.DataFrame.from_dict(features, orient='index')\n",
    "    features_df.to_csv(output_path)\n",
    "    return features_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c65aa800",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_path = 'train_image_features_multilayer_2.csv'\n",
    "\n",
    "if not os.path.exists(train_features_path):\n",
    "    train_img_features_df = extract_image_features_batch(train_df, 'train_images', train_features_path, feature_extractor)\n",
    "else:\n",
    "    train_img_features_df = pd.read_csv(train_features_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a754955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005409</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020625</td>\n",
       "      <td>0.011393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008304</td>\n",
       "      <td>0.020765</td>\n",
       "      <td>0.003993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016222</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.017930</td>\n",
       "      <td>0.003970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015424</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039768</td>\n",
       "      <td>0.012066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009582</td>\n",
       "      <td>0.018250</td>\n",
       "      <td>0.017895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.029877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021935</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042767</td>\n",
       "      <td>0.013229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008133</td>\n",
       "      <td>0.018146</td>\n",
       "      <td>0.015689</td>\n",
       "      <td>0.003610</td>\n",
       "      <td>0.007465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043219</td>\n",
       "      <td>0.027267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041632</td>\n",
       "      <td>0.014679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029391</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009342</td>\n",
       "      <td>0.024435</td>\n",
       "      <td>0.011317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039925</td>\n",
       "      <td>0.022227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044680</td>\n",
       "      <td>0.013551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009510</td>\n",
       "      <td>0.020025</td>\n",
       "      <td>0.018581</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>0.010590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045064</td>\n",
       "      <td>0.029458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70374</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043504</td>\n",
       "      <td>0.013253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027202</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009714</td>\n",
       "      <td>0.017239</td>\n",
       "      <td>0.006776</td>\n",
       "      <td>0.006258</td>\n",
       "      <td>0.014984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046916</td>\n",
       "      <td>0.016415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70375</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008637</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033372</td>\n",
       "      <td>0.017676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011495</td>\n",
       "      <td>0.021976</td>\n",
       "      <td>0.008498</td>\n",
       "      <td>0.004063</td>\n",
       "      <td>0.017001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037098</td>\n",
       "      <td>0.019618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70376</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035437</td>\n",
       "      <td>0.014044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006933</td>\n",
       "      <td>0.016043</td>\n",
       "      <td>0.007201</td>\n",
       "      <td>0.003737</td>\n",
       "      <td>0.012383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038345</td>\n",
       "      <td>0.013832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70377</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011287</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024785</td>\n",
       "      <td>0.013025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003574</td>\n",
       "      <td>0.016109</td>\n",
       "      <td>0.003295</td>\n",
       "      <td>0.004139</td>\n",
       "      <td>0.012210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025680</td>\n",
       "      <td>0.014510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70378</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040479</td>\n",
       "      <td>0.011389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006602</td>\n",
       "      <td>0.015986</td>\n",
       "      <td>0.020225</td>\n",
       "      <td>0.006475</td>\n",
       "      <td>0.007073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044747</td>\n",
       "      <td>0.021376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70213 rows × 512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1    2    3         4    5         6    7         8  \\\n",
       "0      0.0  0.005409  0.0  0.0  0.009460  0.0  0.007700  0.0  0.020625   \n",
       "1      0.0  0.003050  0.0  0.0  0.001893  0.0  0.015424  0.0  0.039768   \n",
       "2      0.0  0.001412  0.0  0.0  0.002201  0.0  0.021935  0.0  0.042767   \n",
       "3      0.0  0.004688  0.0  0.0  0.006735  0.0  0.015019  0.0  0.041632   \n",
       "4      0.0  0.001384  0.0  0.0  0.002471  0.0  0.019506  0.0  0.044680   \n",
       "...    ...       ...  ...  ...       ...  ...       ...  ...       ...   \n",
       "70374  0.0  0.006419  0.0  0.0  0.004811  0.0  0.023222  0.0  0.043504   \n",
       "70375  0.0  0.006930  0.0  0.0  0.008637  0.0  0.019204  0.0  0.033372   \n",
       "70376  0.0  0.006383  0.0  0.0  0.005898  0.0  0.015584  0.0  0.035437   \n",
       "70377  0.0  0.011287  0.0  0.0  0.000000  0.0  0.017004  0.0  0.024785   \n",
       "70378  0.0  0.000954  0.0  0.0  0.001017  0.0  0.016699  0.0  0.040479   \n",
       "\n",
       "              9  ...       502  503       504       505       506       507  \\\n",
       "0      0.011393  ...  0.015672  0.0  0.008304  0.020765  0.003993  0.000000   \n",
       "1      0.012066  ...  0.029899  0.0  0.009582  0.018250  0.017895  0.000000   \n",
       "2      0.013229  ...  0.029253  0.0  0.008133  0.018146  0.015689  0.003610   \n",
       "3      0.014679  ...  0.029391  0.0  0.009342  0.024435  0.011317  0.000000   \n",
       "4      0.013551  ...  0.032837  0.0  0.009510  0.020025  0.018581  0.001886   \n",
       "...         ...  ...       ...  ...       ...       ...       ...       ...   \n",
       "70374  0.013253  ...  0.027202  0.0  0.009714  0.017239  0.006776  0.006258   \n",
       "70375  0.017676  ...  0.030136  0.0  0.011495  0.021976  0.008498  0.004063   \n",
       "70376  0.014044  ...  0.025806  0.0  0.006933  0.016043  0.007201  0.003737   \n",
       "70377  0.013025  ...  0.018171  0.0  0.003574  0.016109  0.003295  0.004139   \n",
       "70378  0.011389  ...  0.028627  0.0  0.006602  0.015986  0.020225  0.006475   \n",
       "\n",
       "            508       509       510       511  \n",
       "0      0.016222  0.000164  0.017930  0.003970  \n",
       "1      0.010045  0.000000  0.039062  0.029877  \n",
       "2      0.007465  0.000000  0.043219  0.027267  \n",
       "3      0.012909  0.000000  0.039925  0.022227  \n",
       "4      0.010590  0.000000  0.045064  0.029458  \n",
       "...         ...       ...       ...       ...  \n",
       "70374  0.014984  0.000000  0.046916  0.016415  \n",
       "70375  0.017001  0.000000  0.037098  0.019618  \n",
       "70376  0.012383  0.000000  0.038345  0.013832  \n",
       "70377  0.012210  0.000000  0.025680  0.014510  \n",
       "70378  0.007073  0.000000  0.044747  0.021376  \n",
       "\n",
       "[70213 rows x 512 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1d765aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge image features with train and test data\n",
    "train_df = train_df.merge(train_img_features_df, left_on='id', right_index=True)\n",
    "\n",
    "\n",
    "# Prepare data for XGBoost\n",
    "train_df['Category'] = train_df['Category'].astype('category').cat.codes\n",
    "X = train_df.drop(columns=['id', 'attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', \n",
    "                           'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10'])\n",
    "y_attributes = train_df[['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', \n",
    "                         'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10']]\n",
    "\n",
    "# Split data for training and validation\n",
    "X_train, X_val, y_train_attrs, y_val_attrs = train_test_split(X, y_attributes, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54486f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33510</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010251</td>\n",
       "      <td>0.023820</td>\n",
       "      <td>0.002353</td>\n",
       "      <td>0.003159</td>\n",
       "      <td>0.016683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032450</td>\n",
       "      <td>0.014590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12486</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019031</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038607</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007356</td>\n",
       "      <td>0.011114</td>\n",
       "      <td>0.012857</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>0.009663</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041517</td>\n",
       "      <td>0.021430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41888</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006233</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027621</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007092</td>\n",
       "      <td>0.020835</td>\n",
       "      <td>0.008168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023707</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026803</td>\n",
       "      <td>0.019949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64755</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003066</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028485</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007496</td>\n",
       "      <td>0.017294</td>\n",
       "      <td>0.015123</td>\n",
       "      <td>0.002104</td>\n",
       "      <td>0.010007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044094</td>\n",
       "      <td>0.021809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43956</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023485</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008073</td>\n",
       "      <td>0.013138</td>\n",
       "      <td>0.016818</td>\n",
       "      <td>0.006505</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047203</td>\n",
       "      <td>0.027273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37194</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007513</td>\n",
       "      <td>0.018332</td>\n",
       "      <td>0.009807</td>\n",
       "      <td>0.002647</td>\n",
       "      <td>0.011420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034423</td>\n",
       "      <td>0.017569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6265</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009649</td>\n",
       "      <td>0.022311</td>\n",
       "      <td>0.007189</td>\n",
       "      <td>0.003242</td>\n",
       "      <td>0.014461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033343</td>\n",
       "      <td>0.018182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54886</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030932</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026910</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008226</td>\n",
       "      <td>0.018314</td>\n",
       "      <td>0.009991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032974</td>\n",
       "      <td>0.018676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017759</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008418</td>\n",
       "      <td>0.015132</td>\n",
       "      <td>0.016607</td>\n",
       "      <td>0.004433</td>\n",
       "      <td>0.009365</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.021308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017224</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007497</td>\n",
       "      <td>0.014173</td>\n",
       "      <td>0.010592</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>0.009935</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040560</td>\n",
       "      <td>0.018529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56170 rows × 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Category    0         1    2    3         4    5         6    7  \\\n",
       "33510         4  0.0  0.004107  0.0  0.0  0.008559  0.0  0.011779  0.0   \n",
       "12486         2  0.0  0.002727  0.0  0.0  0.004705  0.0  0.019031  0.0   \n",
       "41888         4  0.0  0.015173  0.0  0.0  0.012154  0.0  0.006233  0.0   \n",
       "64755         3  0.0  0.001908  0.0  0.0  0.003066  0.0  0.016635  0.0   \n",
       "43956         4  0.0  0.002575  0.0  0.0  0.002628  0.0  0.023485  0.0   \n",
       "...         ...  ...       ...  ...  ...       ...  ...       ...  ...   \n",
       "37194         4  0.0  0.005506  0.0  0.0  0.004087  0.0  0.013905  0.0   \n",
       "6265          1  0.0  0.006141  0.0  0.0  0.006325  0.0  0.015034  0.0   \n",
       "54886         3  0.0  0.008849  0.0  0.0  0.008235  0.0  0.010837  0.0   \n",
       "860           1  0.0  0.000706  0.0  0.0  0.002273  0.0  0.017759  0.0   \n",
       "15795         2  0.0  0.004546  0.0  0.0  0.005584  0.0  0.017224  0.0   \n",
       "\n",
       "              8  ...       502  503       504       505       506       507  \\\n",
       "33510  0.031383  ...  0.028108  0.0  0.010251  0.023820  0.002353  0.003159   \n",
       "12486  0.038607  ...  0.028184  0.0  0.007356  0.011114  0.012857  0.004555   \n",
       "41888  0.028777  ...  0.027621  0.0  0.007092  0.020835  0.008168  0.000000   \n",
       "64755  0.042017  ...  0.028485  0.0  0.007496  0.017294  0.015123  0.002104   \n",
       "43956  0.042978  ...  0.031461  0.0  0.008073  0.013138  0.016818  0.006505   \n",
       "...         ...  ...       ...  ...       ...       ...       ...       ...   \n",
       "37194  0.032411  ...  0.026779  0.0  0.007513  0.018332  0.009807  0.002647   \n",
       "6265   0.035154  ...  0.025459  0.0  0.009649  0.022311  0.007189  0.003242   \n",
       "54886  0.030932  ...  0.026910  0.0  0.008226  0.018314  0.009991  0.000000   \n",
       "860    0.035360  ...  0.024247  0.0  0.008418  0.015132  0.016607  0.004433   \n",
       "15795  0.038871  ...  0.027099  0.0  0.007497  0.014173  0.010592  0.003146   \n",
       "\n",
       "            508  509       510       511  \n",
       "33510  0.016683  0.0  0.032450  0.014590  \n",
       "12486  0.009663  0.0  0.041517  0.021430  \n",
       "41888  0.023707  0.0  0.026803  0.019949  \n",
       "64755  0.010007  0.0  0.044094  0.021809  \n",
       "43956  0.008621  0.0  0.047203  0.027273  \n",
       "...         ...  ...       ...       ...  \n",
       "37194  0.011420  0.0  0.034423  0.017569  \n",
       "6265   0.014461  0.0  0.033343  0.018182  \n",
       "54886  0.017318  0.0  0.032974  0.018676  \n",
       "860    0.009365  0.0  0.039062  0.021308  \n",
       "15795  0.009935  0.0  0.040560  0.018529  \n",
       "\n",
       "[56170 rows x 513 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ceee0306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_xgboost(trial, X_train, y_train, X_val, y_val, num_classes):\n",
    "    # Suggest hyperparameters to be tuned\n",
    "    params = {\n",
    "        'objective': 'multi:softmax',\n",
    "        'num_class': num_classes,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'use_label_encoder': False,\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 1e-3, 10, log=True)\n",
    "    }\n",
    "    \n",
    "    # Train the model\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and calculate F1 score\n",
    "    y_pred = model.predict(X_val)\n",
    "    f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "    \n",
    "    return -f1  # Optuna minimizes, so return negative F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bb98a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-16 23:19:50,797] A new study created in memory with name: no-name-7270a0e8-3a6b-44e9-ac5c-42268fe4d616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing XGBoost model for attr_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ghosl\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [23:19:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-16 23:25:23,738] Trial 0 finished with value: -0.5657634828193087 and parameters: {'eta': 0.052385660243620555, 'max_depth': 9, 'min_child_weight': 0.002463788084111508}. Best is trial 0 with value: -0.5657634828193087.\n",
      "C:\\Users\\Ghosl\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [23:25:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-16 23:28:01,516] Trial 1 finished with value: -0.5585216393898358 and parameters: {'eta': 0.07846989888405989, 'max_depth': 7, 'min_child_weight': 2.605622463295473}. Best is trial 0 with value: -0.5657634828193087.\n",
      "C:\\Users\\Ghosl\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [23:28:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-16 23:31:45,647] Trial 2 finished with value: -0.5666439980968322 and parameters: {'eta': 0.09548627608036607, 'max_depth': 8, 'min_child_weight': 0.23637747683420576}. Best is trial 2 with value: -0.5666439980968322.\n",
      "C:\\Users\\Ghosl\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [23:31:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-16 23:33:16,370] Trial 3 finished with value: -0.5053989569271421 and parameters: {'eta': 0.0797784257693066, 'max_depth': 4, 'min_child_weight': 0.003022286652062018}. Best is trial 2 with value: -0.5666439980968322.\n",
      "C:\\Users\\Ghosl\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [23:33:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-16 23:37:21,401] Trial 4 finished with value: -0.5595116636740771 and parameters: {'eta': 0.046425406766956, 'max_depth': 8, 'min_child_weight': 0.0030924168856991518}. Best is trial 2 with value: -0.5666439980968322.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for attr_1: {'eta': 0.09548627608036607, 'max_depth': 8, 'min_child_weight': 0.23637747683420576}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ghosl\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [23:37:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-16 23:41:07,987] A new study created in memory with name: no-name-615fb4b0-d14e-4521-bf0b-dbef0e66597b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for attr_1: 0.5666\n",
      "\n",
      "Optimizing XGBoost model for attr_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ghosl\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [23:41:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-16 23:45:20,746] Trial 0 finished with value: -0.640963410760646 and parameters: {'eta': 0.010349397990232982, 'max_depth': 10, 'min_child_weight': 0.22545118131346978}. Best is trial 0 with value: -0.640963410760646.\n",
      "C:\\Users\\Ghosl\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [23:45:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[W 2024-11-16 23:45:48,724] Trial 1 failed with parameters: {'eta': 0.17530893673116546, 'max_depth': 4, 'min_child_weight': 1.1784453047233683} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ghosl\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ghosl\\AppData\\Local\\Temp\\ipykernel_15456\\543810778.py\", line 14, in <lambda>\n",
      "    study.optimize(lambda trial: optimize_xgboost(trial, X_train, y_train, X_val, y_val, num_classes), n_trials=5)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ghosl\\AppData\\Local\\Temp\\ipykernel_15456\\2068206841.py\", line 15, in optimize_xgboost\n",
      "    model.fit(X_train, y_train)\n",
      "  File \"C:\\Users\\Ghosl\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ghosl\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1531, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"C:\\Users\\Ghosl\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ghosl\\anaconda3\\Lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"C:\\Users\\Ghosl\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 2101, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2024-11-16 23:45:48,726] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Define an Optuna study\u001b[39;00m\n\u001b[0;32m     13\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: optimize_xgboost(trial, X_train, y_train, X_val, y_val, num_classes), n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Train final model with best parameters\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     _optimize(\n\u001b[0;32m    476\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    477\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    478\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[0;32m    479\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    480\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    481\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[0;32m    482\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    483\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[0;32m    484\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[0;32m    485\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         _optimize_sequential(\n\u001b[0;32m     64\u001b[0m             study,\n\u001b[0;32m     65\u001b[0m             func,\n\u001b[0;32m     66\u001b[0m             n_trials,\n\u001b[0;32m     67\u001b[0m             timeout,\n\u001b[0;32m     68\u001b[0m             catch,\n\u001b[0;32m     69\u001b[0m             callbacks,\n\u001b[0;32m     70\u001b[0m             gc_after_trial,\n\u001b[0;32m     71\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     72\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     73\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m     74\u001b[0m         )\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[15], line 14\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Define an Optuna study\u001b[39;00m\n\u001b[0;32m     13\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: optimize_xgboost(trial, X_train, y_train, X_val, y_val, num_classes), n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Train final model with best parameters\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 15\u001b[0m, in \u001b[0;36moptimize_xgboost\u001b[1;34m(trial, X_train, y_train, X_val, y_val, num_classes)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m---> 15\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Predict and calculate F1 score\u001b[39;00m\n\u001b[0;32m     18\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1531\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1511\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[0;32m   1512\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1513\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1514\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1528\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1529\u001b[0m )\n\u001b[1;32m-> 1531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[0;32m   1532\u001b[0m     params,\n\u001b[0;32m   1533\u001b[0m     train_dmatrix,\n\u001b[0;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_boosting_rounds(),\n\u001b[0;32m   1535\u001b[0m     evals\u001b[38;5;241m=\u001b[39mevals,\n\u001b[0;32m   1536\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping_rounds,\n\u001b[0;32m   1537\u001b[0m     evals_result\u001b[38;5;241m=\u001b[39mevals_result,\n\u001b[0;32m   1538\u001b[0m     obj\u001b[38;5;241m=\u001b[39mobj,\n\u001b[0;32m   1539\u001b[0m     custom_metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[0;32m   1540\u001b[0m     verbose_eval\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   1541\u001b[0m     xgb_model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   1542\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[0;32m   1543\u001b[0m )\n\u001b[0;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m bst\u001b[38;5;241m.\u001b[39mupdate(dtrain, iteration\u001b[38;5;241m=\u001b[39mi, fobj\u001b[38;5;241m=\u001b[39mobj)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2100\u001b[0m     _check_call(\n\u001b[1;32m-> 2101\u001b[0m         _LIB\u001b[38;5;241m.\u001b[39mXGBoosterUpdateOneIter(\n\u001b[0;32m   2102\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, ctypes\u001b[38;5;241m.\u001b[39mc_int(iteration), dtrain\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   2103\u001b[0m         )\n\u001b[0;32m   2104\u001b[0m     )\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train an XGBoost model for each attribute with Optuna optimization\n",
    "optimized_models = {}\n",
    "f1_scores = []\n",
    "\n",
    "for i, attr in enumerate(y_attributes.columns):\n",
    "    print(f\"\\nOptimizing XGBoost model for {attr}\")\n",
    "    \n",
    "    y_train = y_train_attrs[attr]\n",
    "    y_val = y_val_attrs[attr]\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    \n",
    "    # Define an Optuna study\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: optimize_xgboost(trial, X_train, y_train, X_val, y_val, num_classes), n_trials=5)\n",
    "    \n",
    "    print(f\"Best parameters for {attr}: {study.best_params}\")\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    best_params = study.best_params\n",
    "    best_params.update({\n",
    "        'objective': 'multi:softmax',\n",
    "        'num_class': num_classes,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'use_label_encoder': False,\n",
    "    })\n",
    "    \n",
    "    model = xgb.XGBClassifier(**best_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    optimized_models[attr] = model\n",
    "    \n",
    "    # Evaluate final model\n",
    "    y_pred = model.predict(X_val)\n",
    "    f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    print(f\"F1 Score for {attr}: {f1:.4f}\")\n",
    "\n",
    "# Calculate and print average F1 score\n",
    "average_f1_score = np.mean(f1_scores)\n",
    "print(f\"\\nAverage F1 Score across all attributes: {average_f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49f4410",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_path = 'test_image_features_multilayer_2.csv'\n",
    "if not os.path.exists(test_features_path):\n",
    "    test_img_features_df = extract_image_features_batch(test_df, 'test_images', test_features_path, feature_extractor)\n",
    "else:\n",
    "    test_img_features_df = pd.read_csv(test_features_path, index_col=0)\n",
    "test_df = test_df.merge(test_img_features_df, left_on='id', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94ac90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data and make predictions\n",
    "test_df['Category'] = test_df['Category'].astype('category').cat.codes\n",
    "X_test_final = test_df.drop(columns=['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56c33bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48be90fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "for i, (attr, model) in enumerate(models.items()):\n",
    "    print(f\"Making predictions for {attr}\")\n",
    "    predictions[attr] = model.predict(X_test_final)\n",
    "\n",
    "# Decode predictions back to original label values\n",
    "predicted_attributes = {\n",
    "    attr: encoders[attr].inverse_transform(predictions[attr])\n",
    "    for attr in y_attributes.columns\n",
    "}\n",
    "predicted_df = pd.DataFrame(predicted_attributes)\n",
    "\n",
    "# Concatenate predictions with 'id' and 'Category' columns for submission\n",
    "test_predictions = pd.concat([test_df[['id', 'Category']], predicted_df], axis=1)\n",
    "\n",
    "# Map encoded 'Category' values back to original labels\n",
    "original_test_df = pd.read_csv('test.csv')\n",
    "category_mapping = dict(enumerate(original_test_df['Category'].astype('category').cat.categories))\n",
    "test_predictions['Category'] = test_predictions['Category'].map(category_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2843096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to CSV\n",
    "test_predictions.to_csv('submission_vistax_amit_rs_x_finetune_2_multi_3.csv', index=False)\n",
    "print(\"submission_finetuned_rs_x.csv file saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403a766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to extract features from images\n",
    "# def extract_resnet_features(img_path, feature_extractor, target_size=(256, 256)):\n",
    "#     img = image.load_img(img_path, target_size=target_size)\n",
    "#     img_array = image.img_to_array(img)\n",
    "#     img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
    "#     features = feature_extractor.predict(img_array)\n",
    "#     return features.flatten()\n",
    "\n",
    "# # Batch-wise image feature extraction with progress tracking\n",
    "# def extract_image_features_batch(df, img_dir, output_path, feature_extractor):\n",
    "#     features = {}\n",
    "#     for img_id in tqdm(df['id'], desc=\"Extracting Features\"):\n",
    "#         img_path = os.path.join(img_dir, f\"{int(img_id):06}.jpg\")\n",
    "#         if os.path.exists(img_path):\n",
    "#             features[img_id] = extract_resnet_features(img_path, feature_extractor)\n",
    "#     features_df = pd.DataFrame.from_dict(features, orient='index')\n",
    "#     features_df.to_csv(output_path)\n",
    "#     return features_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
